Tasks:
-----

. validate idea to use fc layers instead of distance function to compute the similarity metric
.. compare with euclidean distance
. make a testing interface
. person re-id benchmark
. implement minibatch balancing
. method to get wrong prediction images
. look at the best DL re-id papers and try to recreate them in keras
. read paper about deformable convolutional networks: https://arxiv.org/pdf/1703.06211.pdf
. implement cyclical learning rate: https://arxiv.org/pdf/1506.01186.pdf
. GANs for re-id

other:
. write thesis
. update model latex file with progress on different parts of the model
. update data latex file with info about CASIA
. add noise to gradients: https://arxiv.org/pdf/1511.06807.pdf
. look into different CNN models: https://arxiv.org/pdf/1605.07678.pdf
. look into morphing conv layers: https://arxiv.org/pdf/1701.03281.pdf
. look into dataset augmentation in feature space: https://arxiv.org/pdf/1702.05538.pdf
. look into updating loss function: https://arxiv.org/pdf/1606.09202.pdf
. look into steerable CNN: https://arxiv.org/pdf/1612.08498.pdf
. look into remembering rare events: https://arxiv.org/pdf/1703.03129.pdf
. look into PixelCNN to generate images: https://arxiv.org/pdf/1606.05328.pdf
. look into filter pruning for CNN: https://arxiv.org/pdf/1608.08710.pdf
. look into how LSTMs work
------------------------------------------------------------------------------------------------

===============================================================================================
Second phase: Improve the prototype in experiment cycles
===============================================================================================

14 april
------
. do batch normalization
. implement cyclical learning rate: https://arxiv.org/pdf/1506.01186.pdf
. continue make method for dynamically loading data 
. try out bottlenecks
. do a better cleanup of utils when there's time


13 april
------
. DONE get new CNN weights by training CNN on NICTA data
. DONE experiment with the new CNN weights in the SCN
. DONE clean project utils


12 april
------
. DONE continue try using 2 1D convolutions
. start make method for dynamically loading data 


11 april
------
. start try using 2 1D convolutions
. DONE  make a realistic test dataset
. DONE check if everything is happening correct


10 april
------
. DONE continue writing thesis
. DONE gather more training data
.. training with VIPeR + CUHK1 now: acc=0.64


7 april
------
. continue writing thesis


6 april
------
. DONE start add extra convolutional layers in the human detection CNN
. DONE start freeze CNN layers
. DONE start make method confusion matrix for the siamese results
. DONE start make method for logging experiments
. DONE start compare freezing vs not freezing
.. freezing is slightly better
. DONE start validate idea to use fc layers instead of distance function to compute the similarity metric
.. compare with euclidean distance: euclidean makes it predict everything is negative
. start write thesis


5 april
------
. DONE start clean repository
. DONE start figure out a way to document experiments
.. ideas:
.. make local storage system version controlled by github
.. later build a testing interface on top 

===============================================================================================
First phase: Created basic person re-id prototype
===============================================================================================

5 april
------
. DONE continue making the siamese model work 
. DONE fix issue with loss and accuracy
.. I found out it works with built in keras optimizer, loss and metrics. The 'contrastive loss' as was used in the mnist_siamese example was not appropriate for this problem

4 april 
------
. continue making the siamese model work 
.. there is an issue with the loss and accuracy
. DONE solve the data issue
.. the issue was with displaying the image. fixed with:
    img = Image.fromarray(thing1.astype('uint8')).convert('RGB')


3 april 
------
. continue making the siamese model work 
. issue with displaying the data after loading into memory


31 march
------
. continue making the siamese model work 


30 march
------
. DONE start again with creating a clean CNN
. start making the siamese model in keras
. work with tflearn or keras? let's try keras!
. note: I upgraded to cudnn v6 but then things don't work. downgraded back to v5.1 which fixed the problem
. learned the habit of moving files to /tmp instead of using rm
. keras is super useful. sucks to debug though


29 march
------
. attempt to figure out what's wrong with cnn.py
.. stuff going wrong with the loading of data from checkpoint
.. creating checkpoint works, but loading does not
.. NameError: free variable 'conv1_weights' referenced before assignment in enclosing scope
.. ad hoc fix: load the variables earlier
. new problem: not sure if the variables loaded are the ones that have been trained.


20 feb
------
. added method to see wrong test classifications
. added saving of weights and biases
. attempting to restore with stored weights and biases: issues with the structure, referencing variables before they are created. think it is needed to create a better structured CNN to save and load.
. start with creating a clean CNN


17 feb
------
. trained a bit, added 6th conv layer


16 feb
------
. issues with structuring; work with modded cnn instead



15 feb
------

. clean up the repo a bit

. install gpu ?
DONE . continue make the cnn into siamese cnn

. load the CNN with pre-trained weights from human recognition
.. train the CNN used for human recognition
.. figure out how to store and reload weights
.. figure out how to do it in the SCNN

. do person re-id with images 
.. choose a data-set
.. prepare the data


14 feb
------
. continue make the cnn into siamese cnn


13 feb
------
. start make the cnn into siamese cnn


10 feb
------
. fixed 'data must be of hashable type error'
. made lstm that can accept sequence of images


9 feb
-----
. finish the lstm part of the convolutional RNN
. ran into 'data must be of hashable type error'

notes:
.tf.nn.rnn creates an unrolled graph for a fixed RNN length. That means, if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, you’re unable to pass in longer sequences (> 200) than you’ve originally specified. tf.nn.dynamic_rnn solves this. It uses a tf.While loop to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size.


8 feb
-----
. started with lstm part of the convolutional RNN
. downloaded parts of CASIA

notes:
. 3D convolutions are computationally expensive but they are able to handle sequence of images

7 feb
-----
DONE . research gpu

DONE . figure out why loss becomes NaN DONE
-probably overfitting
-hidden layers can make gradients unstable. use Xavier initialization to fix.
-the variance of the initial values will tend to be too high, causing instability. Also, decreasing the learning rate may help
--Decreasing the learning rate solved the issue
--Xavier initialized: errors go to zero but only if I initialize the conv weights to xavier. initializing the conv biases to xavier leads to more normal errors. not true, the zero thing only happened once apparently...weird. Overall initialization with Xavier leads to lower minibatch loss at the start

DONE . monitoring with tensorboard DONE

DONE . update data latex with info about HDA and hard negative mining


6 feb
-----
. made basic cnn that can distinguish between humans and non-humans
