Tasks:
-----

. validate idea to use fc layers instead of distance function to compute the similarity metric
.. compare with euclidean distance
. make a testing interface
. person re-id benchmark
. implement minibatch balancing
. method to get wrong prediction images
. look at the best DL re-id papers and try to recreate them in keras
. read paper about deformable convolutional networks: https://arxiv.org/pdf/1703.06211.pdf
. GANs for re-id

other:
. write thesis
. update model latex file with progress on different parts of the model
. update data latex file with info about CASIA
. add noise to gradients: https://arxiv.org/pdf/1511.06807.pdf
. look into different CNN models: https://arxiv.org/pdf/1605.07678.pdf
. look into morphing conv layers: https://arxiv.org/pdf/1701.03281.pdf
. look into dataset augmentation in feature space: https://arxiv.org/pdf/1702.05538.pdf
. look into updating loss function: https://arxiv.org/pdf/1606.09202.pdf
. look into steerable CNN: https://arxiv.org/pdf/1612.08498.pdf
. look into remembering rare events: https://arxiv.org/pdf/1703.03129.pdf
. look into PixelCNN to generate images: https://arxiv.org/pdf/1606.05328.pdf
. look into filter pruning for CNN: https://arxiv.org/pdf/1608.08710.pdf
. look into how LSTMs work
------------------------------------------------------------------------------------------------

===============================================================================================
Second phase: Improve the prototype in experiment cycles
===============================================================================================

12 may
------
. DONE fix ranking list of CUHK02
. fix + create lists for caviar
. fix + create lists for grid
. fix + create lists for prid450s
. fix data merging in project_data_handling.py
. fix the data loading bottleneck
.. save all datasets as hdf5
.. adapt data loading methods to load from this
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold 


11 may
------
. DONE fix + create lists for market
.. all the negative combinations take way too much space. so we'll make a selection and make combinations with those.
.. we choose the selection based on the number of identities. if we choose a random selection out of all combinations, there is a big change that multiple images can be present in the set many times. By choosing based on the number of identities we can increase the variation of data that the network sees, and this will be better for generalization. for negatives as well as positives. for each iteration we can create a different subset, making a much better use of all the data we have available while keeping the memory costs to a minumum.
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
How it works
------------
First we create the list of unique positive matches. Then from this list we create combinations of 2 and store the mismatches. 


CREATING THE POSITIVES
----------------------
We start of by having a folder of images of people. 
There are at least 2 images of each person. 
Create a list with all the unique identities <UID> and a list with all the images <ALL>. 
We take a subset of N (random) identities to use for the ranking test. 
Pop the images belonging to the N identities from the list of <ALL> into a list for ranking images <RANKING>. 
Now rename <ALL> to <TRAIN> as this list will be used during training.
For each list, make combinations of 2, so n choose k, where n is the number of images and k is 2. 
For each combo in combinations, only store the pairs that are matches. 
Then we shuffle both lists. 
From each list, select the first occurance of a pair of images where the images are different and belong to the same person. 
Add this pair to the list of positives <POS>. 
Add the id to the seen list <SEEN>.
Repeat the process until we have seen all ids. 
Now there are 2 <POS> lists: <RANK_ALL> and <TRAIN_ALL>.
<RANK_ALL> should have length N and <TRAIN_ALL> should have length [len(UID) - N]

SUMMARY POSITIVES
-----------------
Whenever we repeat the process above, we get a different <RANK_ALL> and <TRAIN_ALL> list with a predefined length, without any duplicate images. These lists contain images from all identities.
This way we can optimally make use of all the data that we have while keeping the memory costs low. We can repeat this process each experiment iteration to generate new subsets to be trained upon. 

CREATING THE NEGATIVES
----------------------
We have <RANK_ALL> and <TRAIN_ALL> lists. 
For each list, make 2 lists: <img1> and <img2> out of each column.
For each image in each list, check if the indices are the same. 
If they are then this is a matching pair and gets the label 1, else the pair gets the label 0.

-- Important difference. 
.. For <RANK_ALL> we want to create 1 final list with all the match and mismatches. 
Save the (mis)matches in the same file.
.. For <TRAIN_ALL> we want to create 2 separate lists, one for all the matches <POSITIVES> and one for all the mismatches <NEGATIVES>.
Save the matches in <POSITIVES> and the mismatches in <NEGATIVES>.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
. fix + create lists for caviar
. fix + create lists for grid
. fix + create lists for prid450s
. fix the data loading bottleneck
.. save all datasets as hdf5
.. adapt data loading methods to load from this
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold 


10 may
------
. DONE fix + create lists for CUHK02
. fix + create lists for market
.. all the negative combinations take way too much space. so we'll make a selection and make combinations with those.
. fix + create lists for caviar
. fix + create lists for grid
. fix + create lists for prid450s
. fix the data loading bottleneck
.. save all datasets as hdf5
.. adapt data loading methods to load from this
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold


9 may
------
. DONE create Docker instance with evertyhing
. figure out how to make the thing run on multiple GPUs
.. halfway there. we have methods to check if a GPU is busy, if not busy assign an experiment to it
.. and pop the experiment from the list. 
.. Now we need to do this in a script that maintains when an experiment has run and assigns the next
. fix the data loading bottleneck
. add CUHK02 + Market-1501 to the pos-neg data
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold


8 may
------
. create Docker instance with evertyhing
. DONE fix the paths to the data
. DONE add more hyperparameters to the experiments
.. add number_of_conv_layers
.. add neural_distance_layers
.. add max_pooling_size
.. add activation_function
.. add loss_function
. DONE setup the experiments in running_experiments.py


5 may
------
. create Docker instance with evertyhing
. add CUHK02 to the datasets
. make list of experiments that are possible with current settings
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold
. remove unused methods from 1) project_utils.py and 2) dynamic_data_loading.py
. add comments to scripts
.. DONE for siamese_cnn_clean.py

4 may
------
. DONE set up better experiment logging
. remove unused methods from 1) project_utils.py and 2) dynamic_data_loading.py
. DONE make that I can get and set variables in project_constants.py
. DONE remove scripts we don't use
. add comments to scripts
.. DONE for siamese_cnn_clean.py


3 may
------
. set up better experiment logging
. remove unused methods from 1) project_utils.py and 2) dynamic_data_loading.py
. make that I can get and set variables in project_constants.py
. remove scripts we don't use
. add comments to scripts

between 20 april and 3 may
------
. I have VPN access to my Thales machine
. NOT using pre-trained weights works really really good.
. Discovered and fixed huge bug: in the train and test set there are multiple images of the same ID. During training the NN learns these. As a result, during testing it performs really well, because even though it hasn't seen the exact pair before, it has seen a veryvery similar pair before and it remembers.
. Realized that this bug implies that in a realistic use case, if you can obtain images of the probe and train the network on them you will bias the network, and as a result achieve better matching and ranking performance. This is pretty awesome.
. Implemented Cumulative Match Characteristic as evaluation metric. Part of this implementation led us to find the bug as described above.
. Now we create a completely isolated test set and remove all related instances from the list of positive instances. This way we have a clean test set and the network is solely tested on its ability to identify a match/mismatch.
. The network trains on a mix of VIPeR and CUHK01 and tests on separate VIPeR and CUHK01 datasets


20 april
------
. continue make method for dynamically loading data 
.. done for the CNN but it's slow and still lots of data needs to be loaded into memory for it to be faster. A big issue is the validation at the end of each epoch. this costs a lot of time.
.. implement with fit_generator
. try out bottlenecks
. do a better cleanup of utils when there's time
. do setup experiments batch that send emails


19 april
------
. remove CLR, BN and make the model simple again
. fix issues
.. when there is no batchnorm in the body, the val_acc and val_loss is the same over all epochs, with or without CLR in the SCN
.. when CLR is too big the performance of the SCN is horrible ~0.2
.. figure out the relationships between BN + CLR + optimizer
.. so far I have trained the CNN using BN + CLR, with and without bias, doing BN after and before relu respectively. I save these weights and init the heads in the SCN with them. Because of a keras issue the BN layers cannot be made 'untrainable'. Or so I thought. It seemed that I was wrong


18 april
------
. DONE do batch normalization
. DONE implement cyclical learning rate: https://arxiv.org/pdf/1506.01186.pdf


14 april
------
. DONE do different normilazation techniques


13 april
------
. DONE get new CNN weights by training CNN on NICTA data
. DONE experiment with the new CNN weights in the SCN
. DONE clean project utils


12 april
------
. DONE continue try using 2 1D convolutions
. start make method for dynamically loading data 


11 april
------
. start try using 2 1D convolutions
. DONE  make a realistic test dataset
. DONE check if everything is happening correct


10 april
------
. DONE continue writing thesis
. DONE gather more training data
.. training with VIPeR + CUHK1 now: acc=0.64


7 april
------
. continue writing thesis


6 april
------
. DONE start add extra convolutional layers in the human detection CNN
. DONE start freeze CNN layers
. DONE start make method confusion matrix for the siamese results
. DONE start make method for logging experiments
. DONE start compare freezing vs not freezing
.. freezing is slightly better
. DONE start validate idea to use fc layers instead of distance function to compute the similarity metric
.. compare with euclidean distance: euclidean makes it predict everything is negative
. start write thesis


5 april
------
. DONE start clean repository
. DONE start figure out a way to document experiments
.. ideas:
.. make local storage system version controlled by github
.. later build a testing interface on top 

===============================================================================================
First phase: Created basic person re-id prototype
===============================================================================================

5 april
------
. DONE continue making the siamese model work 
. DONE fix issue with loss and accuracy
.. I found out it works with built in keras optimizer, loss and metrics. The 'contrastive loss' as was used in the mnist_siamese example was not appropriate for this problem

4 april 
------
. continue making the siamese model work 
.. there is an issue with the loss and accuracy
. DONE solve the data issue
.. the issue was with displaying the image. fixed with:
    img = Image.fromarray(thing1.astype('uint8')).convert('RGB')


3 april 
------
. continue making the siamese model work 
. issue with displaying the data after loading into memory


31 march
------
. continue making the siamese model work 


30 march
------
. DONE start again with creating a clean CNN
. start making the siamese model in keras
. work with tflearn or keras? let's try keras!
. note: I upgraded to cudnn v6 but then things don't work. downgraded back to v5.1 which fixed the problem
. learned the habit of moving files to /tmp instead of using rm
. keras is super useful. sucks to debug though


29 march
------
. attempt to figure out what's wrong with cnn.py
.. stuff going wrong with the loading of data from checkpoint
.. creating checkpoint works, but loading does not
.. NameError: free variable 'conv1_weights' referenced before assignment in enclosing scope
.. ad hoc fix: load the variables earlier
. new problem: not sure if the variables loaded are the ones that have been trained.


20 feb
------
. added method to see wrong test classifications
. added saving of weights and biases
. attempting to restore with stored weights and biases: issues with the structure, referencing variables before they are created. think it is needed to create a better structured CNN to save and load.
. start with creating a clean CNN


17 feb
------
. trained a bit, added 6th conv layer


16 feb
------
. issues with structuring; work with modded cnn instead



15 feb
------

. clean up the repo a bit

. install gpu ?
DONE . continue make the cnn into siamese cnn

. load the CNN with pre-trained weights from human recognition
.. train the CNN used for human recognition
.. figure out how to store and reload weights
.. figure out how to do it in the SCNN

. do person re-id with images 
.. choose a data-set
.. prepare the data


14 feb
------
. continue make the cnn into siamese cnn


13 feb
------
. start make the cnn into siamese cnn


10 feb
------
. fixed 'data must be of hashable type error'
. made lstm that can accept sequence of images


9 feb
-----
. finish the lstm part of the convolutional RNN
. ran into 'data must be of hashable type error'

notes:
.tf.nn.rnn creates an unrolled graph for a fixed RNN length. That means, if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, you’re unable to pass in longer sequences (> 200) than you’ve originally specified. tf.nn.dynamic_rnn solves this. It uses a tf.While loop to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size.


8 feb
-----
. started with lstm part of the convolutional RNN
. downloaded parts of CASIA

notes:
. 3D convolutions are computationally expensive but they are able to handle sequence of images

7 feb
-----
DONE . research gpu

DONE . figure out why loss becomes NaN DONE
-probably overfitting
-hidden layers can make gradients unstable. use Xavier initialization to fix.
-the variance of the initial values will tend to be too high, causing instability. Also, decreasing the learning rate may help
--Decreasing the learning rate solved the issue
--Xavier initialized: errors go to zero but only if I initialize the conv weights to xavier. initializing the conv biases to xavier leads to more normal errors. not true, the zero thing only happened once apparently...weird. Overall initialization with Xavier leads to lower minibatch loss at the start

DONE . monitoring with tensorboard DONE

DONE . update data latex with info about HDA and hard negative mining


6 feb
-----
. made basic cnn that can distinguish between humans and non-humans
