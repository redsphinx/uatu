Tasks:
-----

. validate idea to use fc layers instead of distance function to compute the similarity metric
.. compare with euclidean distance
. make a testing interface
. person re-id benchmark
. implement minibatch balancing
. method to get wrong prediction images
. look at the best DL re-id papers and try to recreate them in keras
. read paper about deformable convolutional networks: https://arxiv.org/pdf/1703.06211.pdf
. GANs for re-id

other:
. write thesis
. update model latex file with progress on different parts of the model
. update data latex file with info about CASIA
. add noise to gradients: https://arxiv.org/pdf/1511.06807.pdf
. look into different CNN models: https://arxiv.org/pdf/1605.07678.pdf
. look into morphing conv layers: https://arxiv.org/pdf/1701.03281.pdf
. look into dataset augmentation in feature space: https://arxiv.org/pdf/1702.05538.pdf
. look into updating loss function: https://arxiv.org/pdf/1606.09202.pdf
. look into steerable CNN: https://arxiv.org/pdf/1612.08498.pdf
. look into remembering rare events: https://arxiv.org/pdf/1703.03129.pdf
. look into PixelCNN to generate images: https://arxiv.org/pdf/1606.05328.pdf
. look into filter pruning for CNN: https://arxiv.org/pdf/1608.08710.pdf
. look into how LSTMs work
------------------------------------------------------------------------------------------------

===============================================================================================
Second phase: Improve the prototype in experiment cycles
===============================================================================================

15 june
-------
. translate from keras to tensorflow
.. make new virtualenv with the latest tensorflow
... things to install: h5py, matplotlib, keras, seaborn, PIL, skimage
	https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.1.0-cp27-none-linux_x86_64.whl
	pip install h5py
	pip install Pillow
	pip install matplotlib
	pip install scikit-image
	pip install seaborn

.. siamese_cnn_clean
.. priming




. fix the datapaths to relative
.. prepare experiment log

. fix the transfer learning pipeline

. fix the ranking stuff

. set up to run on all 4 GPUs



Did some experiments inbetween, figured out that mixing datasets is bad, we should go more transfer learning way.

2 june
------

. DONE implement saving at indicated checkpoints

. start implementing different distance metrics to compare with 'neural distance':
.. DONE euclidean
.. cosine distance metric
.. KISSME(keep it simple and straightforward metric): https://files.icg.tugraz.at/seafhttp/files/2ae5d903-de52-48b2-a0a5-aa3d1de1cd6f/koestinger_cvpr_2012.pdf
.. [RELEVANT] deep hybrid similarity learning: https://arxiv.org/pdf/1702.04858.pdf
.. [optional] MFA (marginal fisher analysis): http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4337772
.. [optional] RankSVM: https://arxiv.org/pdf/1701.06351.pdf

. setup new experiments
.. experiments with SCN

This is to more quickly find out what works and what doesn't. So not to compare with SOTA

>>> 10 epochs = 14 mins with rank=20, batch_size=64
    5 iter + 40 epochs = 5*4*14=280m=4.7h
92. baseline:	
	adjustable.iterations = 5
	adjustable.epochs = 40
92_2. improved baseline:
	adjustable.activation_function = 'elu'
	adjustable.cl_min = 0.00005
	adjustable.cl_max = 0.001
93. baseline [euclidean]
	adjustable.cost_module_type = 'euclidean'
93_2. improved baseline [euclidean]:
	adjustable.activation_function = 'elu'
	adjustable.cl_min = 0.00005
	adjustable.cl_max = 0.001

4*280m=18.7H
started at:             02-06 12:38
expected to finish at:  03-06 07:20

** [TODO] if euclidean sucks, train longer
.. it does suck, I think something is going wrong with the ranking. something that has to do with 0 being distance is small VS. not a match

>>> Depending on which one is better, choose those settings.

[cost_module_type = 'neural_network']

94. adjustable.numfil = 1	#(will take less time)    138m
95. adjustable.pooling_size = [[4,2], [2,2]]        145m
96. adjustable.neural_distance_layers = (128, 256)  240m
97. adjustable.kernel = (5, 5) takes like 5500 sec ~ 92m  ~ 1.57 hours for 1 iteration: way too long


98. adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256)   113m
99. adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]]                                                    111m
100. adjustable.numfil = 2 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256)  144m

adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256) + adjustable.kernel = (5, 5)
adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.kernel = (5, 5)
adjustable.numfil = 2 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256) + adjustable.kernel = (5, 5)

101. adjustable.pooling_type = 'avg_pooling'  241m
102. adjustable.neural_distance = 'absolute'  238m
103. adjustable.neural_distance = 'subtract'  241m
104. adjustable.neural_distance = 'divide'    ~24:42

>>> If neural_distance modifications are making sense:
105. adjustable.numfil = 1 + adjustable.neural_distance = 'absolute'
106. adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance = 'absolute'
107. adjustable.numfil = 1 + adjustable.neural_distance_layers = (128, 256) + adjustable.neural_distance = 'absolute'
108. adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256) + adjustable.neural_distance = 'absolute'

Depending on the results, repeat with numfil = 2

adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256) + adjustable.neural_distance = 'absolute' + adjustable.kernel = (5, 5)
adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256) + adjustable.neural_distance = 'subtract' + adjustable.kernel = (5, 5)
adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256) + adjustable.neural_distance = 'divide' + adjustable.kernel = (5, 5)

[cost_module_type = 'euclidean']
** this has to train enough and may take more time than 'neural_network'
adjustable.numfil = 1	(will take less time)
adjustable.pooling_size = [[4,2], [2,2]]
adjustable.kernel = (5, 5)
adjustable.distance_threshold = 1

adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] 

.. experiments with priming
When we have the best network:
adjustable.awesomeperfection = ???
adjustable.save_inbetween = true

after we save the models and weights:
for each epoch in adjustable.save_points:
adjustable.load_weights_name = '???'
adjustable.load_model_name = '???'
adjustable.prime_epochs = 5

1 june
------
. DONE start create methods for priming

. DONE start fixing code for other distance metrics: 
.. DONE pu.make_confusion_matrix()
.. DONE pu.calculate_CMC()

. DONE start more data

. start implementing different distance metrics to compare with 'neural distance':
.. DONE euclidean
.. cosine distance metric
.. KISSME(keep it simple and straightforward metric): https://lrs.icg.tugraz.at/research/kissme/paper/lrs_icg_koestinger_cvpr_2012.pdf
.. [RELEVANT] deep hybrid similarity learning: https://arxiv.org/pdf/1702.04858.pdf
.. [optional] MFA (marginal fisher analysis): http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4337772
.. [optional] RankSVM: https://arxiv.org/pdf/1701.06351.pdf

. setup new experiments
.. experiments with SCN
.. experiments with priming

29 may
------
. start create methods for priming
.. in siamese_cnn_clean write method to load from saved weights
.. in project_data_handling and dynamic_data_loading to load the correct identities 
.. in siamese_cnn_clean to train on the selected identities
.. put options to use augmented data or not [DEFAULT]

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

HOW PRIMING WORKS

First we train a SCN on the training set. 
We separate the test cases (the identities) and save these to a file. 
After training we save the weights of this SCN.

Then for each ID we train the network again, in addition to the previous training, on a very select subset of training images.
This training subset we will call the prime-train. 
After the SCN is trained on the prime-train, we stop training and we do a forward pass, getting the CMC ranking for that particular ID.

Then we initiate the network with the original trained network.
We repeat the process above for all IDs until we have the complete CMC ranking matrix.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


. DONE write email to payens
. start implement normalization of images: 0-255 to 0-1
. start fixing code for other distance metrics: 
.. pu.make_confusion_matrix()
.. pu.calculate_CMC()
. start implementing different distance metrics to compare with 'neural distance':
.. euclidean
.. cosine distance metric
.. KISSME(keep it simple and straightforward metric): https://lrs.icg.tugraz.at/research/kissme/paper/lrs_icg_koestinger_cvpr_2012.pdf
.. [RELEVANT] deep hybrid similarity learning: https://arxiv.org/pdf/1702.04858.pdf
.. [optional] MFA (marginal fisher analysis): http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4337772
.. [optional] RankSVM: https://arxiv.org/pdf/1701.06351.pdf
. start implementation for different max pooling sizes
.. this means different number of conv layers
. start modify network for video


22 may
------
. start implement normalization of images: 0-255 to 0-1
. start fixing code for other distance metrics: 
.. pu.make_confusion_matrix()
.. pu.calculate_CMC()
. start implementing different distance metrics to compare with 'neural distance':
.. euclidean
.. cosine distance metric
.. KISSME(keep it simple and straightforward metric): https://lrs.icg.tugraz.at/research/kissme/paper/lrs_icg_koestinger_cvpr_2012.pdf
.. [RELEVANT] deep hybrid similarity learning: https://arxiv.org/pdf/1702.04858.pdf
.. [optional] MFA (marginal fisher analysis): http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4337772
.. [optional] RankSVM: https://arxiv.org/pdf/1701.06351.pdf
. start implementation for different max pooling sizes
.. this means different number of conv layers
. start create methods for priming
. start modify network for video


. DONE finish debugging data pipeline. 
.. there is an issue when you leave balancing on when you use all datasets: since all datasets are not the same size, it will take the smallest dataset and copy that amount of pos/neg, so in the end 
.. if you use all datasets, leaving out CUHK01, then you have a training data size of 534. This is stupid, so I've added an option to turn off balancing. 
. overall there seems to be some speedup. the bottlenecks now are 1) when the data gets loaded from the h5 files and 2) when pairs need to be made. But now we are looking through more data and making
.. sure that the data is very diverse. With diverse we mean that the pipeline finds images of all identities to train on. Before, we didn't care about the variety of identities that the network was seeing,
.. so it could have been that images of person_1 were in the training set like 10 times. With the current setup this should not be possible.
. I can't say how much the speedup is objectively. but there is speedup. training is notably faster than before. 
. conclusions from preliminary test with 30 epochs and 5 iters using all data: 
.. takes about 2 hours (7283 sec) to complete 5 iterations of 30 epochs. this is really nice compared to 55 mins(3324 sec) for 1 iteration of 40 epoch. so that's 48s/e now VS. 83s/e before
.. [hypothesis] because market and cuhk02 are overrepresented in the data, these datasets do better on benchmarks
. conclusions compared to 30 epochs, 5 iterations on viper, grid, prid450 and caviar only:
.. hypothesis is rejected: datasets do worse on benchmarks trained on their own data only

17-21 may
------
. tried to make the keras docker work on the devbox, but it did not work. Suspect that it's because of CentOS instead of Ubuntu. 
. got permission from dennis to install virtualenv and just gonna try with that.
. also been debugging the data pipeline


16 may
------
. fix data merging in project_data_handling.py Or in dynamic_data_loading.py
. fix the data loading bottleneck
.. DONE save all datasets as hdf5
.. adapt data loading methods to load from this
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold 
. add methods to do "cheating but not: real world re-id"
. change metric from accuracy to precision
. write methods for subtraction and division of feature tensors in keras

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
How data loading works.

For each dataset we have a folder with 
	- a file with the full path to the location of the original image: these will be considered keys
	- a hdf5 file containing the original images. Each image has a key with the same name as their original path
With ddl.load_specified_datasets() we can return the datasets as readable hdf5 objects
With ddl.create_training_and_ranking_set() we can return the training and ranking selection for the specified dataset
Slices are no longer necessary, since we can load all the data in CPU memory


h5_files = ddl.load_specified_datasets(['dataset_1', 'dataset_2', ..., 'dataset_n'])

For iteration in iterations:
	ranking, training_pos, training_neg = ddl.create_training_and_ranking_set(['dataset_1', 'dataset_2', ..., 'dataset_n'])
	merged_training_pos, merged_training_neg = ddl.merge_datasets(training_pos, training_neg, 'balanced')
	For epoch in epochs:
		train_neg = sample(merged_training_neg)
		train_set = shuffle(concat(train_neg, merged_training_pos))
		train_set = grab_from(h5_files)
		model.fit(train_set)
	
	For rank_set in ranking:
		model.predict()

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^




15 may
------
. DONE fix ranking list of CUHK02
. DONE fix + create lists for caviar
. DONE fix + create lists for grid
. DONE fix + create lists for prid450s


11 may
------
. DONE fix + create lists for market
.. all the negative combinations take way too much space. so we'll make a selection and make combinations with those.
.. we choose the selection based on the number of identities. if we choose a random selection out of all combinations, there is a big change that multiple images can be present in the set many times. By choosing based on the number of identities we can increase the variation of data that the network sees, and this will be better for generalization. for negatives as well as positives. for each iteration we can create a different subset, making a much better use of all the data we have available while keeping the memory costs to a minumum.
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
How it works
------------
First we create the list of unique positive matches. Then from this list we create combinations of 2 and store the mismatches. 


CREATING THE POSITIVES
----------------------
We start of by having a folder of images of people. 
There are at least 2 images of each person. 
Create a list with all the unique identities <UID> and a list with all the images <ALL>. 
We take a subset of N (random) identities to use for the ranking test. 
Pop the images belonging to the N identities from the list of <ALL> into a list for ranking images <RANKING>. 
Now rename <ALL> to <TRAIN> as this list will be used during training.
For each list, make combinations of 2, so n choose k, where n is the number of images and k is 2. 
For each combo in combinations, only store the pairs that are matches. 
Then we shuffle both lists. 
From each list, select the first occurance of a pair of images where the images are different and belong to the same person. 
Add this pair to the list of positives <POS>. 
Add the id to the seen list <SEEN>.
Repeat the process until we have seen all ids. 
Now there are 2 <POS> lists: <RANK_ALL> and <TRAIN_ALL>.
<RANK_ALL> should have length N and <TRAIN_ALL> should have length [len(UID) - N]

SUMMARY POSITIVES
-----------------
Whenever we repeat the process above, we get a different <RANK_ALL> and <TRAIN_ALL> list with a predefined length, without any duplicate images. These lists contain images from all identities.
This way we can optimally make use of all the data that we have while keeping the memory costs low. We can repeat this process each experiment iteration to generate new subsets to be trained upon. 

CREATING THE NEGATIVES
----------------------
We have <RANK_ALL> and <TRAIN_ALL> lists. 
For each list, make 2 lists: <img1> and <img2> out of each column.
For each image in each list, check if the indices are the same. 
If they are then this is a matching pair and gets the label 1, else the pair gets the label 0.

-- Important difference. 
.. For <RANK_ALL> we want to create 1 final list with all the match and mismatches. 
Save the (mis)matches in the same file.
.. For <TRAIN_ALL> we want to create 2 separate lists, one for all the matches <POSITIVES> and one for all the mismatches <NEGATIVES>.
Save the matches in <POSITIVES> and the mismatches in <NEGATIVES>.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
. fix + create lists for caviar
. fix + create lists for grid
. fix + create lists for prid450s
. fix the data loading bottleneck
.. save all datasets as hdf5
.. adapt data loading methods to load from this
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold 


10 may
------
. DONE fix + create lists for CUHK02
. fix + create lists for market
.. all the negative combinations take way too much space. so we'll make a selection and make combinations with those.
. fix + create lists for caviar
. fix + create lists for grid
. fix + create lists for prid450s
. fix the data loading bottleneck
.. save all datasets as hdf5
.. adapt data loading methods to load from this
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold


9 may
------
. DONE create Docker instance with evertyhing
. figure out how to make the thing run on multiple GPUs
.. halfway there. we have methods to check if a GPU is busy, if not busy assign an experiment to it
.. and pop the experiment from the list. 
.. Now we need to do this in a script that maintains when an experiment has run and assigns the next
. fix the data loading bottleneck
. add CUHK02 + Market-1501 to the pos-neg data
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold


8 may
------
. create Docker instance with evertyhing
. DONE fix the paths to the data
. DONE add more hyperparameters to the experiments
.. add number_of_conv_layers
.. add neural_distance_layers
.. add max_pooling_size
.. add activation_function
.. add loss_function
. DONE setup the experiments in running_experiments.py


5 may
------
. create Docker instance with evertyhing
. add CUHK02 to the datasets
. make list of experiments that are possible with current settings
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold
. remove unused methods from 1) project_utils.py and 2) dynamic_data_loading.py
. add comments to scripts
.. DONE for siamese_cnn_clean.py

4 may
------
. DONE set up better experiment logging
. remove unused methods from 1) project_utils.py and 2) dynamic_data_loading.py
. DONE make that I can get and set variables in project_constants.py
. DONE remove scripts we don't use
. add comments to scripts
.. DONE for siamese_cnn_clean.py


3 may
------
. set up better experiment logging
. remove unused methods from 1) project_utils.py and 2) dynamic_data_loading.py
. make that I can get and set variables in project_constants.py
. remove scripts we don't use
. add comments to scripts

between 20 april and 3 may
------
. I have VPN access to my Thales machine
. NOT using pre-trained weights works really really good.
. Discovered and fixed huge bug: in the train and test set there are multiple images of the same ID. During training the NN learns these. As a result, during testing it performs really well, because even though it hasn't seen the exact pair before, it has seen a veryvery similar pair before and it remembers.
. Realized that this bug implies that in a realistic use case, if you can obtain images of the probe and train the network on them you will bias the network, and as a result achieve better matching and ranking performance. This is pretty awesome.
. Implemented Cumulative Match Characteristic as evaluation metric. Part of this implementation led us to find the bug as described above.
. Now we create a completely isolated test set and remove all related instances from the list of positive instances. This way we have a clean test set and the network is solely tested on its ability to identify a match/mismatch.
. The network trains on a mix of VIPeR and CUHK01 and tests on separate VIPeR and CUHK01 datasets


20 april
------
. continue make method for dynamically loading data 
.. done for the CNN but it's slow and still lots of data needs to be loaded into memory for it to be faster. A big issue is the validation at the end of each epoch. this costs a lot of time.
.. implement with fit_generator
. try out bottlenecks
. do a better cleanup of utils when there's time
. do setup experiments batch that send emails


19 april
------
. remove CLR, BN and make the model simple again
. fix issues
.. when there is no batchnorm in the body, the val_acc and val_loss is the same over all epochs, with or without CLR in the SCN
.. when CLR is too big the performance of the SCN is horrible ~0.2
.. figure out the relationships between BN + CLR + optimizer
.. so far I have trained the CNN using BN + CLR, with and without bias, doing BN after and before relu respectively. I save these weights and init the heads in the SCN with them. Because of a keras issue the BN layers cannot be made 'untrainable'. Or so I thought. It seemed that I was wrong


18 april
------
. DONE do batch normalization
. DONE implement cyclical learning rate: https://arxiv.org/pdf/1506.01186.pdf


14 april
------
. DONE do different normilazation techniques


13 april
------
. DONE get new CNN weights by training CNN on NICTA data
. DONE experiment with the new CNN weights in the SCN
. DONE clean project utils


12 april
------
. DONE continue try using 2 1D convolutions
. start make method for dynamically loading data 


11 april
------
. start try using 2 1D convolutions
. DONE  make a realistic test dataset
. DONE check if everything is happening correct


10 april
------
. DONE continue writing thesis
. DONE gather more training data
.. training with VIPeR + CUHK1 now: acc=0.64


7 april
------
. continue writing thesis


6 april
------
. DONE start add extra convolutional layers in the human detection CNN
. DONE start freeze CNN layers
. DONE start make method confusion matrix for the siamese results
. DONE start make method for logging experiments
. DONE start compare freezing vs not freezing
.. freezing is slightly better
. DONE start validate idea to use fc layers instead of distance function to compute the similarity metric
.. compare with euclidean distance: euclidean makes it predict everything is negative
. start write thesis


5 april
------
. DONE start clean repository
. DONE start figure out a way to document experiments
.. ideas:
.. make local storage system version controlled by github
.. later build a testing interface on top 

===============================================================================================
First phase: Created basic person re-id prototype
===============================================================================================

5 april
------
. DONE continue making the siamese model work 
. DONE fix issue with loss and accuracy
.. I found out it works with built in keras optimizer, loss and metrics. The 'contrastive loss' as was used in the mnist_siamese example was not appropriate for this problem

4 april 
------
. continue making the siamese model work 
.. there is an issue with the loss and accuracy
. DONE solve the data issue
.. the issue was with displaying the image. fixed with:
    img = Image.fromarray(thing1.astype('uint8')).convert('RGB')


3 april 
------
. continue making the siamese model work 
. issue with displaying the data after loading into memory


31 march
------
. continue making the siamese model work 


30 march
------
. DONE start again with creating a clean CNN
. start making the siamese model in keras
. work with tflearn or keras? let's try keras!
. note: I upgraded to cudnn v6 but then things don't work. downgraded back to v5.1 which fixed the problem
. learned the habit of moving files to /tmp instead of using rm
. keras is super useful. sucks to debug though


29 march
------
. attempt to figure out what's wrong with cnn.py
.. stuff going wrong with the loading of data from checkpoint
.. creating checkpoint works, but loading does not
.. NameError: free variable 'conv1_weights' referenced before assignment in enclosing scope
.. ad hoc fix: load the variables earlier
. new problem: not sure if the variables loaded are the ones that have been trained.


20 feb
------
. added method to see wrong test classifications
. added saving of weights and biases
. attempting to restore with stored weights and biases: issues with the structure, referencing variables before they are created. think it is needed to create a better structured CNN to save and load.
. start with creating a clean CNN


17 feb
------
. trained a bit, added 6th conv layer


16 feb
------
. issues with structuring; work with modded cnn instead



15 feb
------

. clean up the repo a bit

. install gpu ?
DONE . continue make the cnn into siamese cnn

. load the CNN with pre-trained weights from human recognition
.. train the CNN used for human recognition
.. figure out how to store and reload weights
.. figure out how to do it in the SCNN

. do person re-id with images 
.. choose a data-set
.. prepare the data


14 feb
------
. continue make the cnn into siamese cnn


13 feb
------
. start make the cnn into siamese cnn


10 feb
------
. fixed 'data must be of hashable type error'
. made lstm that can accept sequence of images


9 feb
-----
. finish the lstm part of the convolutional RNN
. ran into 'data must be of hashable type error'

notes:
.tf.nn.rnn creates an unrolled graph for a fixed RNN length. That means, if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, you’re unable to pass in longer sequences (> 200) than you’ve originally specified. tf.nn.dynamic_rnn solves this. It uses a tf.While loop to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size.


8 feb
-----
. started with lstm part of the convolutional RNN
. downloaded parts of CASIA

notes:
. 3D convolutions are computationally expensive but they are able to handle sequence of images

7 feb
-----
DONE . research gpu

DONE . figure out why loss becomes NaN DONE
-probably overfitting
-hidden layers can make gradients unstable. use Xavier initialization to fix.
-the variance of the initial values will tend to be too high, causing instability. Also, decreasing the learning rate may help
--Decreasing the learning rate solved the issue
--Xavier initialized: errors go to zero but only if I initialize the conv weights to xavier. initializing the conv biases to xavier leads to more normal errors. not true, the zero thing only happened once apparently...weird. Overall initialization with Xavier leads to lower minibatch loss at the start

DONE . monitoring with tensorboard DONE

DONE . update data latex with info about HDA and hard negative mining


6 feb
-----
. made basic cnn that can distinguish between humans and non-humans
