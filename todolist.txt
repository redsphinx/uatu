Tasks:
-----

. validate idea to use fc layers instead of distance function to compute the similarity metric
.. compare with euclidean distance
. make a testing interface
. person re-id benchmark
. implement minibatch balancing
. method to get wrong prediction images
. look at the best DL re-id papers and try to recreate them in keras
. read paper about deformable convolutional networks: https://arxiv.org/pdf/1703.06211.pdf
. GANs for re-id

other:
. write thesis
. update model latex file with progress on different parts of the model
. update data latex file with info about CASIA
. add noise to gradients: https://arxiv.org/pdf/1511.06807.pdf
. look into different CNN models: https://arxiv.org/pdf/1605.07678.pdf
. look into morphing conv layers: https://arxiv.org/pdf/1701.03281.pdf
. look into dataset augmentation in feature space: https://arxiv.org/pdf/1702.05538.pdf
. look into updating loss function: https://arxiv.org/pdf/1606.09202.pdf
. look into steerable CNN: https://arxiv.org/pdf/1612.08498.pdf
. look into remembering rare events: https://arxiv.org/pdf/1703.03129.pdf
. look into PixelCNN to generate images: https://arxiv.org/pdf/1606.05328.pdf
. look into filter pruning for CNN: https://arxiv.org/pdf/1608.08710.pdf
. look into how LSTMs work
------------------------------------------------------------------------------------------------

===============================================================================================
Third phase: Make prototype + SCRNN
===============================================================================================

Discussion: what to do about issue using timedistributed + batchnorm?
----------
. could use 3D conv
. or just not use batchnorm
. find replacement for batchnorm
.. make selu + alphadropout work
. don't use timedistributed
.. just use LSTM
.. use conv3D + LSTM
. try K.setlearningphase: https://github.com/fchollet/keras/issues/5934

. User guide
.. scripts with use cases
.. examples
. Dependencies
.. Python libraries
.. Nvidia CUDA drivers + library versions


3 aug
-----
. [DONE] add sideways shuffle to main script
.. loss is going down, but the results are still shit
... debug sideways_shuffle
... run the old script again for sanity
. [SOLVED] performance issue in prid450 is caused by 
	1) removing sideways shuffle 
	2) feeding wrong slice of data containing the labeled pair of images that are part of test set, during training

. write out all the experiments in a script
.. decided to also run experiments on cuhk01
... check if the methods for cuhk01 work
.. wrote ugly way to train, save and the retrain. see if it works
.. make the ranking=100
... in the end run against benchmark
.. turn on sideways shuffle in video
.. figure out sequence number when mixing video data

. start running part of the experiments on this machine
. check with the other guys
.. if possible, start running the video experiments on the other machine

. [HOME] write thesis introduction


31 july
-------
. make experiments
. set up testing script



27 july
-------
. fix for siamese_cnn_video
.. DONE high level stuff
.. low level stuff 

. fix for priming
. (maybe fix for cnn_clean)
. clean up code
. get code ready for experimentation
. prepare experiments


26 july
-------
. continue fixing pipeline for mixing data
. semi-DONE do something such that we can choose ranking numbers for each dataset that we train with
	
	DONE make_pairs_image
	DONE make_pairs_cuhk2 2x
	make_pairs_video

DONE TODO: add ranking_variable parameter to create_training_and_ranking_set	/ make_pairs_image + make_pairs_cuhk02
DONE TODO: add ranking_variable parameter to create_training_and_ranking_set	


. DONE optimize mapping
.. converted things to dictionary


25 july
-------
. continue fixing pipeline for mixing data

TODO: do something such that we can choose ranking numbers for each dataset that we train with

DONE train_network()
DONE get_negative_sample

DONE TODO fix the positives in the samples
DONE get_final_training_data


DONE grab_em_by_the_keys()
DONE create_key_dataset_mapping()
DONE merge_datasets()
DONE create_training_and_ranking_set()
DONE load_datasets_from_h5()


DONE make_pairs_image
DONE make_positive_pairs
DONE make_pairs_cuhk2


24 july
-------
. fix pipelining for mixing data
.. create variables dataset_train and dataset_test, to distinct between training and testing
.. don't mess with ddl.load_datasets_from_h5()

. fix siamese_cnn_{image, video}.py to contain options for mixing
. optimize ranking
. make a clear structure for thesis


###
shit changed a lot. check the commit messages and the comments in the code.

###


21 july
-------

experiment() -> running_experiments_<x>.py
ProjectVariable.datasets -> super_main() -> siamese_cnn_image.py
ProjectVariable.datasets -> load_datasets_from_h5() -> dynamic_data_loading.py == returns list containing h5 datasets
dataset -> create_training_and_ranking_set() -> dynamic_data_loading.py == returns 3 lists: ranking, training_pos, training_neg
training_pos, training_neg -> merge_datasets() -> dynamic_data_loading.py == returns 2 lists: merged_training_pos, merged_training_neg
ranking, merged_training_pos, merged_training_neg -> main() -> siamese_cnn_image.py
becomes final_training_data -> train_network() -> siamese_cnn_image.py 
^^ -> grab_em_by_the_keys() -> dynamic_data_loading.py
^^ -> create_key_dataset_mapping() -> dynamic_data_loading.py



12 july
-------
. DONE adapt confusion matrix calculations such that class balance rate of match/mismatch=0.1
.. think it's done, needs testing
. DONE make 2 new metrics:
.. detection rate (or true positive as gregor calls it) = TP / (TP + FN)
.. false alarm = FP / (FP + TN)
. Run new experiments [home]
. continue fixing priming.py
.. not done there is shit to fix in dynamic_data_loading.py
.. I think it's done. Now we debug!
. look at using MPII dataset for human shizzle
. look into creating dataset with only 1 angle
. do transfer learning with lower learning rate
. check if cosine similarity correctly implemented: http://www.minerazzi.com/tutorials/cosine-similarity-tutorial.pdf
.. flipped the labels in project_utils zero_to_min_one method: a mismatch should be indicated with a -1 because the cosine is a method of similarity and not distance. so the more alike something is, the higher the similarity.
.. run new experiments with cosine omg.
. optimize ranking shit
.. so we can compare video on SOTA benchmarks
. clean dynamic_data_loading.py


For at home: run experiments with selu + alphadropout, they are in running_experiments_3 I think
And some experiments with video stuff

11 july
-------
. DONE make timedistributed srcnn
.. with selu
.. with alphadropout
.. apply to the real code
. run experiments:
.. selu + alphadropout=0.05 + no batchnorm
.. selu + alphadropout=0.05
. check if cosine similarity correctly implemented: http://www.minerazzi.com/tutorials/cosine-similarity-tutorial.pdf
. start fixing priming.py
.. I think we're done. needs to be tested and then we know
.. NOPE not done there is shit to fix in dynamic_data_loading.py
. optimize ranking shit
.. so we can compare video on SOTA benchmarks


10 july
-------
. DONE fix cnn_clean.py
.. NICTA dataset has issues, images are too small
. DONE fix bug in the confusion matrix code
.. was not a bug, (1a,2b) != (1b,2a)
. DONE modify scnn for selu and alphadropout
. fix priming.py

6 july
------
. Trying to recreate the VIPER surpassing SOTA
. When switching from TF to Keras, don't forget to change imports in CLR as well!!
.. 2 things that could be to blame:
		1) merge vs lambda layer - nope
		2) using keras instead of TF - nope
. For a brief moment I thought that maybe the CLR import of keras vs TF was the issue, but then i remembered that in the simple example i am not using CLR at all so this is not the issue.


5 july
------
. Documentation
.. DONE clean + document dynamic_data_loading
... Renamed `project_data_handling.py` to `data_pipeline.py`
. Prototype
.. DONE weights GPU transferable to CPU?
... YES
. Think about experiments that have to be run for the thesis

In general I think the experiments should be centered around the research questions. Take the presentation as the
guideline for the minimum experiments. Do those and if there is time do more.
>> So for now just do the experiments in the presentation x10.

Research Questions:
1. Can we benefit from avoiding an explicit Mahalanobis distance?

Euclidean vs. FC layers
Cosine distance vs. FC layers

1.1. What about differences between FC operations?
Concat vs. absolute difference

2. What is the best strategy for training on multiple datasets?

Mixed data vs. single data
Transfer learning
- with increasingly more data

3. Can we make use of prior information about the probe?

Priming vs. not priming
Priming using learning rate with decay
Also on more datasets






4 july
------
. Documentation
.. clean + document dynamic_data_loading 
.. clean + document project_data_handling
.. make a diagram of dependencies in parallel
. Prototype
.. weights GPU transferable to CPU?
.. CPU transferable to GPU
. SCRNN
.. make a basic version work - debug
.. make a siamese architecture with the figured out network
. Explore 3D convolutions as an alternative
.. DONE implement 3D convolutions in the SRCNN
... memory issues, computer freezes when setting the ranking number at 100. recommend ranking_number = 30


3 july
------
. Documentation
.. clean + document dynamic_data_loading 
.. clean + document project_data_handling
.. make a diagram of dependencies in parallel
. Prototype
.. weights GPU transferable to CPU?
.. CPU transferable to GPU
. SCRNN
.. make a basic version work - debug
.. figured out how to link timedistributed to LSTM
.. problem with timedistributed and batchnomralization
.. problem with keras 2.0.5 not containing selu and alphadropout even though version is latest
.. figured out how to make 3d convolutions work


===============================================================================================
Second phase: Improve the prototype in experiment cycles
===============================================================================================


Inbetween: GUI made mostly.

19 june
-------
. experiments

> rank_number = 100
    > caviar = 36

iterations = 10
epochs = 100

viper baseline                                          e_001 gpu 3
market baseline                                         e_002 gpu 2
cuhk02 baseline                                         e_003 gpu 2
grid baseline                                           e_004 gpu 3
prid450                                                 e_005 gpu 3
caviar                                                  e_006 gpu 3

train on viper  -> market   (full network)              e_007 gpu 2
                -> cuhk02   (full network)              e_008 gpu 2
                -> grid     (full network)              e_009 gpu 3
                -> prid450  (full network)              e_010 gpu 3
                -> caviar   (full network)              e_011 gpu 3

train on viper  -> market   (only classifier)           e_012 gpu 2
                -> cuhk02   (only classifier)           e_013 gpu 2
                -> grid     (only classifier)           e_014 gpu 3
                -> prid450  (only classifier)           e_015 gpu 3
                -> caviar   (only classifier)           e_016 gpu 3

train on cuhk02 -> market   (full network)              e_017 gpu 2
                -> viper    (full network)              e_018 gpu 3
                -> grid     (full network)              e_019 gpu 2
                -> prid450  (full network)              e_020 gpu 2
                -> caviar   (full network)              e_021 gpu 3

train on cuhk02 -> market   (only classifier)           e_022 gpu 2
                -> viper    (only classifier)           e_023 gpu 3
                -> grid     (only classifier)           e_024 gpu 2
                -> prid450  (only classifier)           e_025 gpu 2
                -> caviar   (only classifier)           e_026 gpu 3

train on cuhk02, market -> viper    (full network)      e_027 gpu 2
                        -> grid     (full network)      e_028 gpu 2
                        -> prid450  (full network)      e_029 gpu 3
                        -> caviar   (full network)      e_030 gpu 3

train on cuhk02, market -> viper    (only classifier)   e_031 gpu 2
                        -> grid     (only classifier)   e_032 gpu 2
                        -> prid450  (only classifier)   e_033 gpu 3
                        -> caviar   (only classifier)   e_034 gpu 3

train on market and save, then prime    (full network)                      e_035 gpu 3
train on market and save, then prime    (only_classifier)                   e_036 gpu 3
train on market and save, then prime (only_classifier) prime_epochs=10      e_037 gpu 3


With lowered learning rates


16 june
-------
. translate from keras to tensorflow
..	make new virtualenv with the latest tensorflow
... 	things to install: h5py, matplotlib, keras, seaborn, PIL, skimage
		https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.1.0-cp27-none-linux_x86_64.whl
		pip install h5py
		pip install Pillow
		pip install matplotlib
		pip install scikit-image
		pip install seaborn
.. siamese_cnn_clean
.. priming (TODO)
.. Figure out how to solve issue with 1) merging and 2) batchnormalization

. fix the transfer learning pipeline
..	Decided that data will no longer be mixed. Network is trained on 1 dataset at a time because of batchnorm.
	If the network is trained one dataset at a time we can use all the data from the previously data used for training
	so we only need to store the ranking file of the last (eval) dataset. 
..	Change the way we train the scnn
...		Use the different trainable_ variables to play with what gets frozen and when.
..	fix the ranking stuff
...	Decided that ranking number will be ID/2. Create ranking dictionary in project_constants.py
.. I think DONE for siamese and priming
... NOPE lol still bugs, mainly with ranking. FIX these


.. if we only want to train, set adjustable.ranking_number = 2. But never only train with CUHK02
... how we gonna train with only CUHK02 for a comparison huh
... oh wait we don't adjustable.only_train kekeke
... IT WORKS LIKE THIS

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
HOW RANKING WORKS NOW

you adjust `adjustable.ranking_number` to 1 of 2 options:
1) 'half'
	this takes the value in pc.RANKING_DICT[name_of_datset]
2) <int>
	This sets the ranking number to said int

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
thing = np.zeros((100,100))

for row in range(900):
	start = time.time()
	thing[row] = [i[0] for i in sorted(enumerate(a[row]), key=lambda x: x[1], reverse=True)]
	list_form = thing[row].tolist()
	num = list_form.index(row)
	time.sleep(10)
	print(time.time() - start)
	

b = sorted(enumerate(a[row]), key=lambda x: x[1], reverse=True)
	print(np.shape(b))
	for i in b:
		thing[row] = i[0]


50 = 40s
100 = 53s
316 = 442s, 413s = 8 mins
500 = 1923
915 = 

.. debugging:
1)	scn.super_main() on ['cuhk02']
	- no saving, just train
	- ranking_number = 5, 20, 80
	- with saving
	- FIXED
2)	scn.super_main() on ['viper']
	- no saving, just train
	- ranking_number = 5, 20, 80, 'half'
	- with saving
	- FIXED


in P3 032_, 077_ is causing an issue because there are only 3 images of it
so we remove it

fullpathimages:
/home/gabi/Documents/datasets/CUHK/cropped_CUHK2/P3/all/032_12752.png
/home/gabi/Documents/datasets/CUHK/cropped_CUHK2/P3/all/032_12800.png
/home/gabi/Documents/datasets/CUHK/cropped_CUHK2/P3/all/032_12880.png

idall:
032_
032_
032_

short iamgenames:
032_12752.png
032_12800.png
032_12880.png

swapped:
+home+gabi+Documents+datasets+CUHK+cropped_CUHK2+P3+all+032_12752.png
+home+gabi+Documents+datasets+CUHK+cropped_CUHK2+P3+all+032_12800.png
+home+gabi+Documents+datasets+CUHK+cropped_CUHK2+P3+all+032_12880.png

unique:
032_


. DONE fix the datapaths to relative
. prepare experiment log

. set up to run on all 4 GPUs

. setup experiments
viper
cuhk02
market
grid
prid450
caviar



15 june
-------
. translate from keras to tensorflow
..	make new virtualenv with the latest tensorflow
... 	things to install: h5py, matplotlib, keras, seaborn, PIL, skimage
		https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.1.0-cp27-none-linux_x86_64.whl
		pip install h5py
		pip install Pillow
		pip install matplotlib
		pip install scikit-image
		pip install seaborn
.. siamese_cnn_clean
.. priming (TODO)
.. Figure out how to solve issue with 1) merging and 2) batchnormalization

. fix the transfer learning pipeline
..	Decided that data will no longer be mixed. Network is trained on 1 dataset at a time because of batchnorm.
	If the network is trained one dataset at a time we can use all the data from the previously data used for training
	so we only need to store the ranking file of the last (eval) dataset. 
..	Change the way we train the scnn
...		Use the different trainable_ variables to play with what gets frozen and when.
..	fix the ranking stuff
...	Decided that ranking number will be ID/2. Create ranking dictionary in project_constants.py
.. I think DONE for siamese and priming


.. if we only want to train, set adjustable.ranking_number = 2. But never only train with CUHK02
... how we gonna train with only CUHK02 for a comparison huh
... oh wait we don't adjustable.only_train kekeke

. DONE fix the datapaths to relative
. prepare experiment log

. set up to run on all 4 GPUs

. setup experiments
viper
cuhk02
market
grid
prid450
caviar



Did some experiments inbetween, figured out that mixing datasets is bad, we should go more transfer learning way.

2 june
------

. DONE implement saving at indicated checkpoints

. start implementing different distance metrics to compare with 'neural distance':
.. DONE euclidean
.. cosine distance metric
.. KISSME(keep it simple and straightforward metric): https://files.icg.tugraz.at/seafhttp/files/2ae5d903-de52-48b2-a0a5-aa3d1de1cd6f/koestinger_cvpr_2012.pdf
.. [RELEVANT] deep hybrid similarity learning: https://arxiv.org/pdf/1702.04858.pdf
.. [optional] MFA (marginal fisher analysis): http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4337772
.. [optional] RankSVM: https://arxiv.org/pdf/1701.06351.pdf

. setup new experiments
.. experiments with SCN

This is to more quickly find out what works and what doesn't. So not to compare with SOTA

>>> 10 epochs = 14 mins with rank=20, batch_size=64
    5 iter + 40 epochs = 5*4*14=280m=4.7h
92. baseline:	
	adjustable.iterations = 5
	adjustable.epochs = 40
92_2. improved baseline:
	adjustable.activation_function = 'elu'
	adjustable.cl_min = 0.00005
	adjustable.cl_max = 0.001
93. baseline [euclidean]
	adjustable.cost_module_type = 'euclidean'
93_2. improved baseline [euclidean]:
	adjustable.activation_function = 'elu'
	adjustable.cl_min = 0.00005
	adjustable.cl_max = 0.001

4*280m=18.7H
started at:             02-06 12:38
expected to finish at:  03-06 07:20

** [TODO] if euclidean sucks, train longer
.. it does suck, I think something is going wrong with the ranking. something that has to do with 0 being distance is small VS. not a match

>>> Depending on which one is better, choose those settings.

[cost_module_type = 'neural_network']

94. adjustable.numfil = 1	#(will take less time)    138m
95. adjustable.pooling_size = [[4,2], [2,2]]        145m
96. adjustable.neural_distance_layers = (128, 256)  240m
97. adjustable.kernel = (5, 5) takes like 5500 sec ~ 92m  ~ 1.57 hours for 1 iteration: way too long


98. adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256)   113m
99. adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]]                                                    111m
100. adjustable.numfil = 2 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256)  144m

adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256) + adjustable.kernel = (5, 5)
adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.kernel = (5, 5)
adjustable.numfil = 2 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256) + adjustable.kernel = (5, 5)

101. adjustable.pooling_type = 'avg_pooling'  241m
102. adjustable.neural_distance = 'absolute'  238m
103. adjustable.neural_distance = 'subtract'  241m
104. adjustable.neural_distance = 'divide'    ~24:42

>>> If neural_distance modifications are making sense:
105. adjustable.numfil = 1 + adjustable.neural_distance = 'absolute'
106. adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance = 'absolute'
107. adjustable.numfil = 1 + adjustable.neural_distance_layers = (128, 256) + adjustable.neural_distance = 'absolute'
108. adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256) + adjustable.neural_distance = 'absolute'

Depending on the results, repeat with numfil = 2

adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256) + adjustable.neural_distance = 'absolute' + adjustable.kernel = (5, 5)
adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256) + adjustable.neural_distance = 'subtract' + adjustable.kernel = (5, 5)
adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] + adjustable.neural_distance_layers = (128, 256) + adjustable.neural_distance = 'divide' + adjustable.kernel = (5, 5)

[cost_module_type = 'euclidean']
** this has to train enough and may take more time than 'neural_network'
adjustable.numfil = 1	(will take less time)
adjustable.pooling_size = [[4,2], [2,2]]
adjustable.kernel = (5, 5)
adjustable.distance_threshold = 1

adjustable.numfil = 1 + adjustable.pooling_size = [[4,2], [2,2]] 

.. experiments with priming
When we have the best network:
adjustable.awesomeperfection = ???
adjustable.save_inbetween = true

after we save the models and weights:
for each epoch in adjustable.save_points:
adjustable.load_weights_name = '???'
adjustable.load_model_name = '???'
adjustable.prime_epochs = 5

1 june
------
. DONE start create methods for priming

. DONE start fixing code for other distance metrics: 
.. DONE pu.make_confusion_matrix()
.. DONE pu.calculate_CMC()

. DONE start more data

. start implementing different distance metrics to compare with 'neural distance':
.. DONE euclidean
.. cosine distance metric
.. KISSME(keep it simple and straightforward metric): https://lrs.icg.tugraz.at/research/kissme/paper/lrs_icg_koestinger_cvpr_2012.pdf
.. [RELEVANT] deep hybrid similarity learning: https://arxiv.org/pdf/1702.04858.pdf
.. [optional] MFA (marginal fisher analysis): http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4337772
.. [optional] RankSVM: https://arxiv.org/pdf/1701.06351.pdf

. setup new experiments
.. experiments with SCN
.. experiments with priming

29 may
------
. start create methods for priming
.. in siamese_cnn_clean write method to load from saved weights
.. in project_data_handling and dynamic_data_loading to load the correct identities 
.. in siamese_cnn_clean to train on the selected identities
.. put options to use augmented data or not [DEFAULT]

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

HOW PRIMING WORKS

First we train a SCN on the training set. 
We separate the test cases (the identities) and save these to a file. 
After training we save the weights of this SCN.

Then for each ID we train the network again, in addition to the previous training, on a very select subset of training images.
This training subset we will call the prime-train. 
After the SCN is trained on the prime-train, we stop training and we do a forward pass, getting the CMC ranking for that particular ID.

Then we initiate the network with the original trained network.
We repeat the process above for all IDs until we have the complete CMC ranking matrix.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


. DONE write email to payens
. start implement normalization of images: 0-255 to 0-1
. start fixing code for other distance metrics: 
.. pu.make_confusion_matrix()
.. pu.calculate_CMC()
. start implementing different distance metrics to compare with 'neural distance':
.. euclidean
.. cosine distance metric
.. KISSME(keep it simple and straightforward metric): https://lrs.icg.tugraz.at/research/kissme/paper/lrs_icg_koestinger_cvpr_2012.pdf
.. [RELEVANT] deep hybrid similarity learning: https://arxiv.org/pdf/1702.04858.pdf
.. [optional] MFA (marginal fisher analysis): http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4337772
.. [optional] RankSVM: https://arxiv.org/pdf/1701.06351.pdf
. start implementation for different max pooling sizes
.. this means different number of conv layers
. start modify network for video


22 may
------
. start implement normalization of images: 0-255 to 0-1
. start fixing code for other distance metrics: 
.. pu.make_confusion_matrix()
.. pu.calculate_CMC()
. start implementing different distance metrics to compare with 'neural distance':
.. euclidean
.. cosine distance metric
.. KISSME(keep it simple and straightforward metric): https://lrs.icg.tugraz.at/research/kissme/paper/lrs_icg_koestinger_cvpr_2012.pdf
.. [RELEVANT] deep hybrid similarity learning: https://arxiv.org/pdf/1702.04858.pdf
.. [optional] MFA (marginal fisher analysis): http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4337772
.. [optional] RankSVM: https://arxiv.org/pdf/1701.06351.pdf
. start implementation for different max pooling sizes
.. this means different number of conv layers
. start create methods for priming
. start modify network for video


. DONE finish debugging data pipeline. 
.. there is an issue when you leave balancing on when you use all datasets: since all datasets are not the same size, it will take the smallest dataset and copy that amount of pos/neg, so in the end 
.. if you use all datasets, leaving out CUHK01, then you have a training data size of 534. This is stupid, so I've added an option to turn off balancing. 
. overall there seems to be some speedup. the bottlenecks now are 1) when the data gets loaded from the h5 files and 2) when pairs need to be made. But now we are looking through more data and making
.. sure that the data is very diverse. With diverse we mean that the pipeline finds images of all identities to train on. Before, we didn't care about the variety of identities that the network was seeing,
.. so it could have been that images of person_1 were in the training set like 10 times. With the current setup this should not be possible.
. I can't say how much the speedup is objectively. but there is speedup. training is notably faster than before. 
. conclusions from preliminary test with 30 epochs and 5 iters using all data: 
.. takes about 2 hours (7283 sec) to complete 5 iterations of 30 epochs. this is really nice compared to 55 mins(3324 sec) for 1 iteration of 40 epoch. so that's 48s/e now VS. 83s/e before
.. [hypothesis] because market and cuhk02 are overrepresented in the data, these datasets do better on benchmarks
. conclusions compared to 30 epochs, 5 iterations on viper, grid, prid450 and caviar only:
.. hypothesis is rejected: datasets do worse on benchmarks trained on their own data only

17-21 may
------
. tried to make the keras docker work on the devbox, but it did not work. Suspect that it's because of CentOS instead of Ubuntu. 
. got permission from dennis to install virtualenv and just gonna try with that.
. also been debugging the data pipeline


16 may
------
. fix data merging in project_data_handling.py Or in dynamic_data_loading.py
. fix the data loading bottleneck
.. DONE save all datasets as hdf5
.. adapt data loading methods to load from this
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold 
. add methods to do "cheating but not: real world re-id"
. change metric from accuracy to precision
. write methods for subtraction and division of feature tensors in keras

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
How data loading works.

For each dataset we have a folder with 
	- a file with the full path to the location of the original image: these will be considered keys
	- a hdf5 file containing the original images. Each image has a key with the same name as their original path
With ddl.load_specified_datasets() we can return the datasets as readable hdf5 objects
With ddl.create_training_and_ranking_set() we can return the training and ranking selection for the specified dataset
Slices are no longer necessary, since we can load all the data in CPU memory


h5_files = ddl.load_specified_datasets(['dataset_1', 'dataset_2', ..., 'dataset_n'])

For iteration in iterations:
	ranking, training_pos, training_neg = ddl.create_training_and_ranking_set(['dataset_1', 'dataset_2', ..., 'dataset_n'])
	merged_training_pos, merged_training_neg = ddl.merge_datasets(training_pos, training_neg, 'balanced')
	For epoch in epochs:
		train_neg = sample(merged_training_neg)
		train_set = shuffle(concat(train_neg, merged_training_pos))
		train_set = grab_from(h5_files)
		model.fit(train_set)
	
	For rank_set in ranking:
		model.predict()

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^




15 may
------
. DONE fix ranking list of CUHK02
. DONE fix + create lists for caviar
. DONE fix + create lists for grid
. DONE fix + create lists for prid450s


11 may
------
. DONE fix + create lists for market
.. all the negative combinations take way too much space. so we'll make a selection and make combinations with those.
.. we choose the selection based on the number of identities. if we choose a random selection out of all combinations, there is a big change that multiple images can be present in the set many times. By choosing based on the number of identities we can increase the variation of data that the network sees, and this will be better for generalization. for negatives as well as positives. for each iteration we can create a different subset, making a much better use of all the data we have available while keeping the memory costs to a minumum.
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
How it works
------------
First we create the list of unique positive matches. Then from this list we create combinations of 2 and store the mismatches. 


CREATING THE POSITIVES
----------------------
We start of by having a folder of images of people. 
There are at least 2 images of each person. 
Create a list with all the unique identities <UID> and a list with all the images <ALL>. 
We take a subset of N (random) identities to use for the ranking test. 
Pop the images belonging to the N identities from the list of <ALL> into a list for ranking images <RANKING>. 
Now rename <ALL> to <TRAIN> as this list will be used during training.
For each list, make combinations of 2, so n choose k, where n is the number of images and k is 2. 
For each combo in combinations, only store the pairs that are matches. 
Then we shuffle both lists. 
From each list, select the first occurance of a pair of images where the images are different and belong to the same person. 
Add this pair to the list of positives <POS>. 
Add the id to the seen list <SEEN>.
Repeat the process until we have seen all ids. 
Now there are 2 <POS> lists: <RANK_ALL> and <TRAIN_ALL>.
<RANK_ALL> should have length N and <TRAIN_ALL> should have length [len(UID) - N]

SUMMARY POSITIVES
-----------------
Whenever we repeat the process above, we get a different <RANK_ALL> and <TRAIN_ALL> list with a predefined length, without any duplicate images. These lists contain images from all identities.
This way we can optimally make use of all the data that we have while keeping the memory costs low. We can repeat this process each experiment iteration to generate new subsets to be trained upon. 

CREATING THE NEGATIVES
----------------------
We have <RANK_ALL> and <TRAIN_ALL> lists. 
For each list, make 2 lists: <img1> and <img2> out of each column.
For each image in each list, check if the indices are the same. 
If they are then this is a matching pair and gets the label 1, else the pair gets the label 0.

-- Important difference. 
.. For <RANK_ALL> we want to create 1 final list with all the match and mismatches. 
Save the (mis)matches in the same file.
.. For <TRAIN_ALL> we want to create 2 separate lists, one for all the matches <POSITIVES> and one for all the mismatches <NEGATIVES>.
Save the matches in <POSITIVES> and the mismatches in <NEGATIVES>.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
. fix + create lists for caviar
. fix + create lists for grid
. fix + create lists for prid450s
. fix the data loading bottleneck
.. save all datasets as hdf5
.. adapt data loading methods to load from this
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold 


10 may
------
. DONE fix + create lists for CUHK02
. fix + create lists for market
.. all the negative combinations take way too much space. so we'll make a selection and make combinations with those.
. fix + create lists for caviar
. fix + create lists for grid
. fix + create lists for prid450s
. fix the data loading bottleneck
.. save all datasets as hdf5
.. adapt data loading methods to load from this
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold


9 may
------
. DONE create Docker instance with evertyhing
. figure out how to make the thing run on multiple GPUs
.. halfway there. we have methods to check if a GPU is busy, if not busy assign an experiment to it
.. and pop the experiment from the list. 
.. Now we need to do this in a script that maintains when an experiment has run and assigns the next
. fix the data loading bottleneck
. add CUHK02 + Market-1501 to the pos-neg data
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold


8 may
------
. create Docker instance with evertyhing
. DONE fix the paths to the data
. DONE add more hyperparameters to the experiments
.. add number_of_conv_layers
.. add neural_distance_layers
.. add max_pooling_size
.. add activation_function
.. add loss_function
. DONE setup the experiments in running_experiments.py


5 may
------
. create Docker instance with evertyhing
. add CUHK02 to the datasets
. make list of experiments that are possible with current settings
. figure out how to make euclidean distance work
.. also with calculate_CMC(). the ranking should be reversed: sort on smallest first
.. also with make_confusion_matrix() decide good distance to set as threshold
. remove unused methods from 1) project_utils.py and 2) dynamic_data_loading.py
. add comments to scripts
.. DONE for siamese_cnn_clean.py

4 may
------
. DONE set up better experiment logging
. remove unused methods from 1) project_utils.py and 2) dynamic_data_loading.py
. DONE make that I can get and set variables in project_constants.py
. DONE remove scripts we don't use
. add comments to scripts
.. DONE for siamese_cnn_clean.py


3 may
------
. set up better experiment logging
. remove unused methods from 1) project_utils.py and 2) dynamic_data_loading.py
. make that I can get and set variables in project_constants.py
. remove scripts we don't use
. add comments to scripts

between 20 april and 3 may
------
. I have VPN access to my Thales machine
. NOT using pre-trained weights works really really good.
. Discovered and fixed huge bug: in the train and test set there are multiple images of the same ID. During training the NN learns these. As a result, during testing it performs really well, because even though it hasn't seen the exact pair before, it has seen a veryvery similar pair before and it remembers.
. Realized that this bug implies that in a realistic use case, if you can obtain images of the probe and train the network on them you will bias the network, and as a result achieve better matching and ranking performance. This is pretty awesome.
. Implemented Cumulative Match Characteristic as evaluation metric. Part of this implementation led us to find the bug as described above.
. Now we create a completely isolated test set and remove all related instances from the list of positive instances. This way we have a clean test set and the network is solely tested on its ability to identify a match/mismatch.
. The network trains on a mix of VIPeR and CUHK01 and tests on separate VIPeR and CUHK01 datasets


20 april
------
. continue make method for dynamically loading data 
.. done for the CNN but it's slow and still lots of data needs to be loaded into memory for it to be faster. A big issue is the validation at the end of each epoch. this costs a lot of time.
.. implement with fit_generator
. try out bottlenecks
. do a better cleanup of utils when there's time
. do setup experiments batch that send emails


19 april
------
. remove CLR, BN and make the model simple again
. fix issues
.. when there is no batchnorm in the body, the val_acc and val_loss is the same over all epochs, with or without CLR in the SCN
.. when CLR is too big the performance of the SCN is horrible ~0.2
.. figure out the relationships between BN + CLR + optimizer
.. so far I have trained the CNN using BN + CLR, with and without bias, doing BN after and before relu respectively. I save these weights and init the heads in the SCN with them. Because of a keras issue the BN layers cannot be made 'untrainable'. Or so I thought. It seemed that I was wrong


18 april
------
. DONE do batch normalization
. DONE implement cyclical learning rate: https://arxiv.org/pdf/1506.01186.pdf


14 april
------
. DONE do different normilazation techniques


13 april
------
. DONE get new CNN weights by training CNN on NICTA data
. DONE experiment with the new CNN weights in the SCN
. DONE clean project utils


12 april
------
. DONE continue try using 2 1D convolutions
. start make method for dynamically loading data 


11 april
------
. start try using 2 1D convolutions
. DONE  make a realistic test dataset
. DONE check if everything is happening correct


10 april
------
. DONE continue writing thesis
. DONE gather more training data
.. training with VIPeR + CUHK1 now: acc=0.64


7 april
------
. continue writing thesis


6 april
------
. DONE start add extra convolutional layers in the human detection CNN
. DONE start freeze CNN layers
. DONE start make method confusion matrix for the siamese results
. DONE start make method for logging experiments
. DONE start compare freezing vs not freezing
.. freezing is slightly better
. DONE start validate idea to use fc layers instead of distance function to compute the similarity metric
.. compare with euclidean distance: euclidean makes it predict everything is negative
. start write thesis


5 april
------
. DONE start clean repository
. DONE start figure out a way to document experiments
.. ideas:
.. make local storage system version controlled by github
.. later build a testing interface on top 

===============================================================================================
First phase: Created basic person re-id prototype
===============================================================================================

5 april
------
. DONE continue making the siamese model work 
. DONE fix issue with loss and accuracy
.. I found out it works with built in keras optimizer, loss and metrics. The 'contrastive loss' as was used in the mnist_siamese example was not appropriate for this problem

4 april 
------
. continue making the siamese model work 
.. there is an issue with the loss and accuracy
. DONE solve the data issue
.. the issue was with displaying the image. fixed with:
    img = Image.fromarray(thing1.astype('uint8')).convert('RGB')


3 april 
------
. continue making the siamese model work 
. issue with displaying the data after loading into memory


31 march
------
. continue making the siamese model work 


30 march
------
. DONE start again with creating a clean CNN
. start making the siamese model in keras
. work with tflearn or keras? let's try keras!
. note: I upgraded to cudnn v6 but then things don't work. downgraded back to v5.1 which fixed the problem
. learned the habit of moving files to /tmp instead of using rm
. keras is super useful. sucks to debug though


29 march
------
. attempt to figure out what's wrong with cnn.py
.. stuff going wrong with the loading of data from checkpoint
.. creating checkpoint works, but loading does not
.. NameError: free variable 'conv1_weights' referenced before assignment in enclosing scope
.. ad hoc fix: load the variables earlier
. new problem: not sure if the variables loaded are the ones that have been trained.


20 feb
------
. added method to see wrong test classifications
. added saving of weights and biases
. attempting to restore with stored weights and biases: issues with the structure, referencing variables before they are created. think it is needed to create a better structured CNN to save and load.
. start with creating a clean CNN


17 feb
------
. trained a bit, added 6th conv layer


16 feb
------
. issues with structuring; work with modded cnn instead



15 feb
------

. clean up the repo a bit

. install gpu ?
DONE . continue make the cnn into siamese cnn

. load the CNN with pre-trained weights from human recognition
.. train the CNN used for human recognition
.. figure out how to store and reload weights
.. figure out how to do it in the SCNN

. do person re-id with images 
.. choose a data-set
.. prepare the data


14 feb
------
. continue make the cnn into siamese cnn


13 feb
------
. start make the cnn into siamese cnn


10 feb
------
. fixed 'data must be of hashable type error'
. made lstm that can accept sequence of images


9 feb
-----
. finish the lstm part of the convolutional RNN
. ran into 'data must be of hashable type error'

notes:
.tf.nn.rnn creates an unrolled graph for a fixed RNN length. That means, if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, you’re unable to pass in longer sequences (> 200) than you’ve originally specified. tf.nn.dynamic_rnn solves this. It uses a tf.While loop to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size.


8 feb
-----
. started with lstm part of the convolutional RNN
. downloaded parts of CASIA

notes:
. 3D convolutions are computationally expensive but they are able to handle sequence of images

7 feb
-----
DONE . research gpu

DONE . figure out why loss becomes NaN DONE
-probably overfitting
-hidden layers can make gradients unstable. use Xavier initialization to fix.
-the variance of the initial values will tend to be too high, causing instability. Also, decreasing the learning rate may help
--Decreasing the learning rate solved the issue
--Xavier initialized: errors go to zero but only if I initialize the conv weights to xavier. initializing the conv biases to xavier leads to more normal errors. not true, the zero thing only happened once apparently...weird. Overall initialization with Xavier leads to lower minibatch loss at the start

DONE . monitoring with tensorboard DONE

DONE . update data latex with info about HDA and hard negative mining


6 feb
-----
. made basic cnn that can distinguish between humans and non-humans
