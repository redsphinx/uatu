Tasks:
-----
implementation:

. make a testing interface

. implement cyclical learning rate: https://arxiv.org/pdf/1506.01186.pdf

. person re-id benchmark

. make the lstm part of the convolutional RNN

. connect the cnn with the lstm
. make data pipeline
. implement a different architecture for both
. make architecture work with continuous video stream

research:
. good architecture for person identification
. GANs for re-id

other:
. update model latex file with progress on different parts of the model
. update data latex file with info about CASIA
------------------------------------------------------------------------------------------------
5 april
------
. continue making the siamese model work 
. fix issue with loss and accuracy


4 april 
------
. continue making the siamese model work 
.. there is an issue with the loss and accuracy
. DONE solve the data issue
.. the issue was with displaying the image. fixed with:
    img = Image.fromarray(thing1.astype('uint8')).convert('RGB')


3 april 
------
. continue making the siamese model work 
. issue with displaying the data after loading into memory


31 march
------
. continue making the siamese model work 


30 march
------
. DONE start again with creating a clean CNN
. start making the siamese model in keras
. work with tflearn or keras? let's try keras!
. note: I upgraded to cudnn v6 but then things don't work. downgraded back to v5.1 which fixed the problem
. learned the habit of moving files to /tmp instead of using rm
. keras is super useful. sucks to debug though


29 march
------
. attempt to figure out what's wrong with cnn.py
.. stuff going wrong with the loading of data from checkpoint
.. creating checkpoint works, but loading does not
.. NameError: free variable 'conv1_weights' referenced before assignment in enclosing scope
.. ad hoc fix: load the variables earlier
. new problem: not sure if the variables loaded are the ones that have been trained.


20 feb
------
. added method to see wrong test classifications
. added saving of weights and biases
. attempting to restore with stored weights and biases: issues with the structure, referencing variables before they are created. think it is needed to create a better structured CNN to save and load.
. start with creating a clean CNN


17 feb
------
. trained a bit, added 6th conv layer


16 feb
------
. issues with structuring; work with modded cnn instead



15 feb
------

. clean up the repo a bit

. install gpu ?
DONE . continue make the cnn into siamese cnn

. load the CNN with pre-trained weights from human recognition
.. train the CNN used for human recognition
.. figure out how to store and reload weights
.. figure out how to do it in the SCNN

. do person re-id with images 
.. choose a data-set
.. prepare the data


14 feb
------
. continue make the cnn into siamese cnn


13 feb
------
. start make the cnn into siamese cnn


10 feb
------
. fixed 'data must be of hashable type error'
. made lstm that can accept sequence of images


9 feb
-----
. finish the lstm part of the convolutional RNN
. ran into 'data must be of hashable type error'

notes:
.tf.nn.rnn creates an unrolled graph for a fixed RNN length. That means, if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, you’re unable to pass in longer sequences (> 200) than you’ve originally specified. tf.nn.dynamic_rnn solves this. It uses a tf.While loop to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size.


8 feb
-----
. started with lstm part of the convolutional RNN
. downloaded parts of CASIA

notes:
. 3D convolutions are computationally expensive but they are able to handle sequence of images

7 feb
-----
DONE . research gpu

DONE . figure out why loss becomes NaN DONE
-probably overfitting
-hidden layers can make gradients unstable. use Xavier initialization to fix.
-the variance of the initial values will tend to be too high, causing instability. Also, decreasing the learning rate may help
--Decreasing the learning rate solved the issue
--Xavier initialized: errors go to zero but only if I initialize the conv weights to xavier. initializing the conv biases to xavier leads to more normal errors. not true, the zero thing only happened once apparently...weird. Overall initialization with Xavier leads to lower minibatch loss at the start

DONE . monitoring with tensorboard DONE

DONE . update data latex with info about HDA and hard negative mining


6 feb
-----
. made basic cnn that can distinguish between humans and non-humans
