experiments for the presentation

(1) Transfer of CNN weights to SCNN
	running_experiments_1, experiment_log_1
	28:training SCN with CNN weights: 2D filters 32, BN_lr_0-0001. use CL 0.00001-0.0001
	vs.
	31.SCNN: random weight initialization
	
(2) Usage of CLR
	running_experiments_1, experiment_log_1
	33.SCNN: RIW, BN, no CL, lr=0.00001
	34.SCNN: RIW, BN, no CL, lr=0.0001
	vs.
	38.SCNN: RIW, BN, CL, lr=0.00001-0.0001
	
(3) Different types of neural distance
	running_experiments_1, experiment_log_1, from line 2607
	57. baseline (concatenate)
	58. neural distance: add
	59. neural distance: multiply

(4) Using ELU instead of ReLU
	running_experiments_1, experiment_log_1
	78. combo: elu + CLR min=0.00005, max=0.0005
	vs.
	57. baseline

(5) Different types of neural distance
	running_experiments_1, experiment_log_1
	92. baseline: debugged network
	vs.
	102. neural_distance=absolute
	vs
	103. neural_distance=subtract
	vs
	104. neural_distance=divide
	
(6) Reducing number of filters
	running_experiments_1, experiment_log_1
	105. adjustable.numfil = 1 + neural_distance=absolute
	
(7) Training in order on all data
	running_experiments_1, experiment_log_1
	109. saving network with config. 105 each 100 epochs. preparation for priming experiments
	
(8) Priming at different epochs
 	running_experiments_1, experiment_log_1
 	111 - 130
 	
(9) Train on 1 dataset only
 	running_experiments_1, experiment_log_1
 	161 - 166
 
(10)Transfer on data with rank 20
 	running_experiments_1, experiment_log_1
 	lines 5881 - 5931
 	
(11)Network trained on ALL the data, test on each test set individually, rank 20
 	running_experiments_1, experiment_log_1
 	after training on all data in order test on all ranking test sets
 	lines 5955 - 5982
 	
(12)Transfer + priming
	running_experiments_1, experiment_log_1
	lines 5985 - 6095

----------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------

(13)Transfer learning
	running_experiments_2, log_0.txt
	001 - 034

----------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------

(14)Training on mixed data, rank 100
	running_experiments_2, experiment_log_2
	improved baseline SCNN setup 105, minus caviar: rank=100
	
(15)SOTAs
	running_experiments_2, experiment_log_2
	train on viper only: rank=632
	train on prid450 only: rank=225
	train on caviar only: rank=36
	train on grid only: rank=125
	
(16)new SOTAs
	running_experiments_2, log_0.txt
	038 - 046

(17)Problems with euclidean, cosine
	running_experiments_1, experiment_log_1


http://watchingthewatchers.weebly.com/the-netherlands.html#.WUlHxnYrLmE	
	

Text

Hello everyone, my name is Gabi and I will be telling a little something about how deep learning is used in surveillance systems.

Big brother may or may not be watching.

I say that it is.

Fun facts about surveillance in the Netherlands

According to the Dutch website Sargasso, there are at least 200000 surveillance cameras watching over the inhabitants of the Netherlands. Since privately owened cameras are not mandatory to be reported, it it very likely that this number is a lot higher. According to other sources this number may be as high as 1 million cameras. This includes cameras in supermarkets, schools or business areas. 

These cameras are being used for many types of surveillance, for example traffic surveillance, tracking people of interest and also some very specific things such as detecting if luggage is being left unattended in a public space.

However, big brother is not always watching. As the number of cameras used for surveillence is constantly increasing, this becomes a burden for people who need to keep an eye on all these camera feeds. Cameras are getting very cheap and are easy to install in virtually any imaginable space. It should come as no suprise that the demand for the automization of surveillance is also increasing. 

This is where Artificial Intelligence comes in. It may seem that AI is a magic box that can do everything, but actually it is quite difficult to make things work robustly. For example in surveillance systems, there is no AI (yet) that can do many tasks. Currently we create AI that is able to deal with 1 clearly defined task at a time. Various examples of these tasks are:
- person tracking
- person identification
- vehicle tracking
- object detection

In this talk I will be talking about 1 specific problem that we will automate:
- person re-identification

Person identification is when we say "oh look that is Bob, hi Bob!". This is where the AI has learned the identity of Bob and stores representations of the identity of Bob in a database. This is easy to do in closed environments where everybody's identity is known.

But imagine that we have 100 random strange people on the camera at any given moment, such as a crowd in a public space, where it is simply unfeasible to store everybody's ID in a giant database. Also imagine that we have a couple of cameras observing this space from various angles. Then imagine that there is a human security guard being observing these cameras, and he or she sees a person doing something weird, like punch another person and then run away. This security has seen the person who commited the assault and then has to keep an eye on all the cameras to see where this person might be. Since the identity of the person is not known beforehand but has only just been observed, we only have a couple of reference frames of this person. And then the task is to find this person again on the other cameras. This is known as person re-identification.

In literature we say that, 

given the probe, we have to find its match in the gallery of candidates.

In our case we want to create an automated surveillance system that can do person re-identification in an area where there are no more than 100 people on the camera at the same time. The target scenario is where we have a non-cluttered background with optimally just 1 person at a time on the video under more or less the same angle.

Classically we have computer vision algorithms that deal with this. But recently it has been shown that deep learning is capable of outperforming classic computer vision methods when it comes to things like detecting objects, mainly because we don't have to specify hand-crafted features anymore. Deep learning is capable of detecting dicriminating features by itself. 

Which is why we will be using deep learning to solve this problem.

While the main objective is to create a deep learning model that can perform person re-id in the specific scenario, we will also aim to solve some problems that are relevant in person re-id literature. Answering these questions will also lead to a better model that can be used in practice.

Research questions

We deal with 3 research questions:

(1) Can we avoid the use of an explicit Mahalanobis distance metric?
(2) How can we optimally make use of all datasets available to us?
(3) How can we make use of the fact that we will have some minor knowledge of the probe?

In the rest of my presentation I will talk about why these research questions deserve an answer and what I have done so far to answer them.

Big brother will not be mentioned anymore. Sadface

First of all I will explain briefly what is deep learning, since it gets mentioned all the time in the media. 

This is a neural network. It's a network that is composed of small units called nodes and these are connected to each other. Each connection has a weigth that indicate how important the connection is. Nodes are typically organized in layers. And when we stack these layers we get a deep neural network. Deep learning refers to this stacking of simple units to form a hierarchical structure. The concept is heavily related to the use of Artificial Neural Networks, but it does not solely apply to neural networks.

So now you know what is deep learning. 

Let's move on to the first research question:
(1) Can we avoid the use of an explicit Mahalanobis distance metric?

Where does this problem come from and is it even a problem to begin with. It all has to do with the problem formulation. Previously I mentioned that the question posed by person re-identification is: 'given a probe, find the match in the gallery of candidates'. We can write this problem as <formula>. In terms of Mahalanobis distance we would like to compute the distance between <a> and <b>. We write this as follows <formula>. <Explain formula> So the greater the distance, the less similar <a> and <b> are. When the Mahalanobis distance is used in combination with deep learning, we use a CNN to extract feature vectors from image of <a> and <b>. Then we compute the distance between feature vectors of <a> and <b>. This is awesome and it works in general but BUT we need to specify the inverse covariance matrix. And assuming that the network will converge we need to set a threshold. This brings with it some problems. First of all we don't know at which point the network will converge. And also it brings with it the problem of choosing an appropriate threshold. Most of the time the euclidean distance is chosen. This distance can grow unbounded making it hard to select a threshold.

Several papers deal with this in a couple of different ways. A solution to the first problem of selecting covariance matrix is proposed by KISSME guys where S gets learned. But whatever I don't think it's elegant and why don't we just use deep learning and neural networks all the way.

Instead we propose a more 'holistic' neural network way of dealing with things. Instead of formulating the problem as a distance problem we formulate it as a classification problem.
We can rewrite as <formula>. Explain <formula>. Something something order invariant. 

To see if our methods works we test it and compare with using the euclidean distance.
See results. 

With this we conclude that our method is cool and it kinda works on some level. On this benchmark it even outperforms state of the art. However on other benchmarks it does not. We think it works because the dataset is pretty nice. I think if the dataset IRL is something like this then we are awesome.

That wraps up Q1. Now let's go to the second research question. 
(2) How can we optimally make use of all datasets available to us?

So in lots of papers they pretrain on imagenet and stuff but there's a shitton of data alltogether in the re-id repos but all the individual datasets are quite small for deep learning. We all know that the more data the better when using deep learning. Some papers tried to use more data that is available from the re-id datasets in this and that way. We like to draw our inspiration from that by proposing the following changes based on these experiments. Also don't mix data because batchnorm will mess shit up because the data comes from different distributions. 

Explain experiments. 

Ok cool that was Q2. Hope you guys are still with me. Now the third and last one. 
(3) How can we make use of the fact that we will have some minor knowledge of the probe?

We realized that when we trained the network with similar images the results went up by a bajillion. We've been trying to recreate it in a realistic way since. Sometimes it works. Sometimes it doesn't. Dunno for sure why. Something something difficulties of transfer learning.

Here are experiments showing what I said. I interpret them this and that way.

Ok nice you guys survived that. 

Now let's see if there is time left and I will explain the main architecture because now some shit makes sense since I explained them.

We use a siamese network because the data comes in pairs and it's totally natural given the formulation of the problem. 

Here is a picture of it. We use 6 conv layers and 3 FC layers in the end. So all in all it's a pretty simple network, so that's cool. And it's portable and easy to train which helps when you wanna make shit work IRL and not everyone has a GPU cluster ready to go. 

Some fun facts about the network.

We train with ELU instead of ReLU because it's way cooler, because of <reasons> mentioned in the paper and shit. Here look at these experiments, they prove that ELUs are cooler.

Also we use cyclical learning rate, which is awesome. Explain shit in paper of CLR. Here look at experiments where we show it works better than LR with decay.

So I've been talking for like 15 min straight. Let's wrap it up and summarize, make some conclusions. Big brother is watching you, but if you wanna evade him then do the following:
- wear different colors of clothing and change them regularly if you're running away. 
- grab an object and dont look like you are walking
- 

Also say that I have 2,5 months left in my internship and we are gonna put recurrent units in the network to experiment with pairs of video. And parallel to that we will make a prototype that will work on a stream of real video. 

yee

fin












	
	

